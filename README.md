# Alpaca Stream Ingestion

> **A complete real-time data engineering pipeline for learning and rapid prototyping**

Local-first streaming data platform that ingests live stock market data, processes it through Kafka and Spark, and stores it in modern data lakehouse (Iceberg) and analytics (Pinot) systems - all running on Minikube without any cloud dependencies.

---

## ğŸ¯ Purpose

This project is designed for:
- **Learning**: Hands-on experience with modern data engineering tools (Kafka, Spark, Iceberg, Pinot)
- **Rapid Prototyping**: Test streaming patterns and transformations locally before cloud deployment
- **No Cloud Lock-in**: Everything runs on your machine using Minikube
- **Real Data**: Connect to Alpaca's free tier for live stock market data, or use built-in sample generators

Perfect for data engineers learning streaming patterns, students exploring distributed systems, or professionals prototyping data pipelines.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Alpaca WebSocketâ”‚  (Live IEX Stock Data)
â”‚  or Sample Gen   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache Kafka     â”‚  (Message Broker - Strimzi Operator)
â”‚  Topic: iex_data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PySpark Streamingâ”‚  (Stream Processing)
â”‚   - Flatten JSON   â”‚
â”‚   - Transform      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Iceberg  â”‚  â”‚  Apache Pinot    â”‚
â”‚ (Data Lakehouse)â”‚  â”‚  (Real-time      â”‚
â”‚                 â”‚  â”‚   Analytics)     â”‚
â”‚ Storage: MinIO  â”‚  â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Flow:** WebSocket â†’ Kafka â†’ Spark â†’ (Iceberg + Pinot)

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Infrastructure** | Minikube + Docker | Local Kubernetes cluster |
| **Messaging** | Apache Kafka (Strimzi) | Distributed message broker |
| **Stream Processing** | PySpark 3.5.1 | Real-time data transformation |
| **Data Lakehouse** | Apache Iceberg | ACID-compliant table format with time-travel |
| **Analytics Database** | Apache Pinot | Low-latency OLAP queries |
| **Object Storage** | MinIO | S3-compatible storage for Iceberg |
| **Data Source** | Alpaca WebSocket API | Live stock market data (IEX feed) |
| **Language** | Python 3.10+ | Primary development language |

---

## ğŸ“ Project Structure

```
alpaca_stream_ingestion/
â”œâ”€â”€ extract/                    # Data ingestion layer
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ alpaca_ws_updated_conf.py   # WebSocket â†’ Kafka producer
â”‚   â”‚   â””â”€â”€ conf_reader.py              # Configuration loader
â”‚   â”œâ”€â”€ admin/
â”‚   â”‚   â”œâ”€â”€ create_kafka_topic.py       # Topic management utility
â”‚   â”‚   â””â”€â”€ kafka_consumer_stdout.py    # Debug consumer
â”‚   â””â”€â”€ history/
â”‚       â””â”€â”€ simulate_stream_from_history.py  # Historical data replay
â”‚
â”œâ”€â”€ transform/                  # Stream processing layer
â”‚   â”œâ”€â”€ spark_streaming_flattener_cli.py    # Main Spark streaming job (CLI)
â”‚   â”œâ”€â”€ spark_kafka_to_iceberg.py           # Kafka â†’ Iceberg writer
â”‚   â””â”€â”€ run_pyspark_streamer.sh             # Spark job launcher
â”‚
â”œâ”€â”€ load/                       # Query and analytics layer
â”‚   â”œâ”€â”€ pinot_qeury_display.py  # Pinot query utilities
â”‚   â””â”€â”€ create.py               # Table creation helpers
â”‚
â”œâ”€â”€ minikube/                   # Kubernetes manifests
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ 00-kafka_ns.yaml                # Kafka namespace
â”‚   â”‚   â”œâ”€â”€ 01-stimzi_operator.yaml         # Strimzi operator
â”‚   â”‚   â”œâ”€â”€ 02-kafka_deploy.yaml            # Kafka cluster definition
â”‚   â”‚   â””â”€â”€ sample_event_generators/
â”‚   â”‚       â””â”€â”€ stream_producer_cli.py      # Standalone sample data generator
â”‚   â”œâ”€â”€ minio/                              # MinIO deployment manifests
â”‚   â”œâ”€â”€ pinot/                              # Pinot deployment (Helm values)
â”‚   â”œâ”€â”€ spark/                              # Spark operator manifests
â”‚   â””â”€â”€ extractor_deploy/                   # WebSocket app K8s deployment
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”œâ”€â”€ iceberg/                    # Iceberg table definitions
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ SETUP.md                # Installation & configuration guide
â”‚   â”œâ”€â”€ PIPELINE.md             # Pipeline components deep dive
â”‚   â”œâ”€â”€ USAGE.md                # Operations & examples
â”‚   â””â”€â”€ TROUBLESHOOTING.md      # Common issues & solutions
â”‚
â”œâ”€â”€ test_new_2.sh              # PRIMARY: Automated infrastructure setup
â”œâ”€â”€ cron_trigger.sh            # Periodic job scheduler
â”œâ”€â”€ pyproject.toml             # Python dependencies
â”œâ”€â”€ STRIMZI_KAFKA_CONNECT_GUIDE.md  # Kafka Connect setup guide
â””â”€â”€ README.md                  # This file
```

---

## ğŸš€ Quick Start

### Prerequisites
- **Hardware**: 8 CPU cores, 15GB RAM minimum
- **OS**: Ubuntu Linux (22.04+) or similar
- **Optional**: Alpaca API credentials for live data ([free tier](https://alpaca.markets))

### Automated Setup (5 minutes)

```bash
# Clone the repository
git clone <repository-url>
cd alpaca_stream_ingestion

# Run the automated setup script
chmod +x test_new_2.sh

# Full setup: installs dependencies, starts Minikube, deploys all services
./test_new_2.sh --setup-infra --setup-minikube --setup-kafka --setup-minio --setup-pinot

# Start sample data generator (no credentials needed)
python minikube/kafka/sample_event_generators/stream_producer_cli.py \
  --kafka-brokers localhost:9092 \
  --topic iex_data \
  --symbols AAPL GOOGL MSFT \
  --batch-size 50 \
  --interval 2

# Start Spark streaming job
python transform/spark_streaming_flattener_cli.py \
  --kafka-brokers localhost:9092 \
  --source-topic iex_data \
  --dest-topic flattened_stocks \
  --output-mode both
```

**That's it!** Your pipeline is now running. Access:
- **Pinot Console**: http://localhost:9000 (query analytics)
- **MinIO Console**: http://localhost:9001 (view storage)
- **Spark UI**: http://localhost:4040 (monitoring)

For detailed setup instructions, see **[Setup Guide](docs/SETUP.md)**.

---

## ğŸ’¡ Key Usage Examples

### Stream Live Stock Data

```bash
# Option 1: Use sample generator (no API keys needed)
python minikube/kafka/sample_event_generators/stream_producer_cli.py \
  --kafka-brokers localhost:9092 \
  --topic iex_data \
  --symbols AAPL GOOGL MSFT AMZN TSLA \
  --batch-size 100 \
  --interval 2

# Option 2: Connect to live Alpaca WebSocket (requires credentials)
export ALPACA_KEY="your_api_key"
export ALPACA_SECRET="your_api_secret"
./test_new_2.sh --setup-app
```

### Query Real-time Analytics (Pinot)

```sql
-- Access Pinot console at http://localhost:9000

-- Count trades by symbol
SELECT S as symbol, COUNT(*) as trade_count
FROM flattened_stocks
GROUP BY S
ORDER BY trade_count DESC
LIMIT 10;

-- Average price by symbol
SELECT S as symbol, AVG(p) as avg_price
FROM flattened_stocks
GROUP BY S;
```

### Query Historical Data (Iceberg)

```bash
# Start Spark SQL shell
spark-sql \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0 \
  --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog

# Query latest data
SELECT * FROM local.db.flattened_stocks ORDER BY timestamp DESC LIMIT 10;

# Time-travel query
SELECT * FROM local.db.flattened_stocks VERSION AS OF 123456789;
```

For more examples, see **[Usage Guide](docs/USAGE.md)**.

---

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| **[Setup Guide](docs/SETUP.md)** | Installation, prerequisites, manual setup steps |
| **[Pipeline Components](docs/PIPELINE.md)** | Deep dive into extractors, processors, and storage |
| **[Usage & Operations](docs/USAGE.md)** | Common operations, monitoring, query examples |
| **[Troubleshooting](docs/TROUBLESHOOTING.md)** | Common issues and solutions |
| **[Kafka Connect Guide](STRIMZI_KAFKA_CONNECT_GUIDE.md)** | Strimzi Kafka Connect setup |

---

## ğŸ“ What You'll Learn

This project provides hands-on experience with:

- **Stream Ingestion**: WebSocket â†’ Kafka producer patterns
- **Message Brokers**: Kafka topics, partitions, consumer groups
- **Stream Processing**: Spark Structured Streaming, transformations
- **Data Lakehouse**: Iceberg ACID transactions, time-travel, schema evolution
- **Real-time Analytics**: Pinot indexing, OLAP query optimization
- **Kubernetes**: Deploying stateful applications, operators (Strimzi)
- **Object Storage**: S3-compatible storage with MinIO

**Suggested Next Steps:**
- Add windowed aggregations in Spark (e.g., 5-minute VWAP)
- Implement late data handling and watermarking
- Create Pinot real-time tables with complex indexes
- Experiment with Iceberg partition evolution
- Build custom Spark transformations (joins, sessionization)

---

## ğŸ”§ Common Commands

```bash
# Minikube
minikube start
minikube stop
minikube status

# Check Kafka topics
kubectl exec -it my-cluster-kafka-0 -n kafka -- bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 --list

# Monitor Kafka consumer lag
kubectl exec -it my-cluster-kafka-0 -n kafka -- bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 --describe --group spark-kafka-streaming

# View Spark streaming UI
open http://localhost:4040

# Access Pinot query console
open http://localhost:9000
```

See **[Usage Guide](docs/USAGE.md)** for complete command reference.

---

## ğŸ› Troubleshooting

**Common issues:**
- **Snappy compression error**: Install `libsnappy-dev` and `python-snappy`
- **Minikube out of resources**: Restart with `--cpus=12 --memory=20000`
- **Kafka pod not starting**: Check logs with `kubectl logs -n kafka my-cluster-kafka-0`
- **S3 connection error**: Verify MinIO port-forward is active

See **[Troubleshooting Guide](docs/TROUBLESHOOTING.md)** for detailed solutions.

---

## ğŸ“š External Resources

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [PySpark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)
- [Apache Pinot Documentation](https://docs.pinot.apache.org/)
- [Strimzi Kafka Operator](https://strimzi.io/docs/operators/latest/overview.html)
- [Alpaca API Documentation](https://alpaca.markets/docs/)

---

## ğŸ“ License

This project is for educational and learning purposes. Feel free to use, modify, and extend for your own learning and prototyping needs.

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! This is a learning-focused project, so improvements to documentation, additional examples, and new pipeline patterns are especially appreciated.

---

**Happy Streaming! ğŸš€**
