  
  
  helm install my-spark oci://registry-1.docker.io/bitnamicharts/spark
  helm upgrade my-spark oci://registry-1.docker.io/bitnamicharts/spark -f spark-values.yaml

minikube start --mount --mount-string="/nvmewd/minikube_mount:/shr"
minikube docker daemon
eval $(minikube docker-env)

minikube start --mount --mount-string="/home/kumararpita/alpaca_stream_ingestion/minikube:/home/kumararpita/alpaca_stream_ingestion/minikube"

minikube start --mount --mount-string="/home/kumararpita/alpaca_stream_ingestion/minikube:/mnt/data"

#BELOW COMMANDS WERE USED TO BUILD SPARK IMAGES FOR KUBERNETES
docker build -t spark:v3.5.2.2  -f kubernetes/dockerfiles/spark/Dockerfile .
docker build   --build-arg base_img=spark:v3.5.2.2   -t pyspark:v3.5.2.3 -f kubernetes/dockerfiles/spark/bindings/python/Dockerfile_pyspark  .
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  --master k8s://$(minikube ip):8443 \
  --deploy-mode cluster \
  --name  \
  --conf spark.kubernetes.container.image=spark:v3.5.2.2 \
  --conf spark.kubernetes.context=minikube \
  --conf spark.kubernetes.driver.pod.name=pf-demo17 \
  --conf spark.kubernetes.namespace=spark \
  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
  --conf spark.ui.enabled=false \
  --conf spark.eventLog.enabled=true \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --conf spark.eventLog.dir=/tmp \
  local:///opt/python/spark_kafka_test.py


spark-submit \
  --class Test \
  --master k8s://192.168.49.2:8443 \
  --deploy-mode cluster \
  --name spark-stream3 \
  --conf spark.kubernetes.container.image=spark:v3.5.2.2 \
  --conf spark.kubernetes.context=minikube \
  --conf spark.kubernetes.namespace=spark \
  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
  --conf spark.kubernetes.driver.volumes.hostPath.data.mount.path=/shr \
  --conf spark.kubernetes.driver.volumes.hostPath.data.options.path=/shr\
  --conf spark.kubernetes.driver.volumes.hostPath.data.options.type=Directory \
  --conf spark.kubernetes.executor.volumes.hostPath.data.mount.path=/shr \
  --conf spark.kubernetes.executor.volumes.hostPath.data.options.path=/shr \
  --conf spark.kubernetes.executor.volumes.hostPath.data.options.type=Directory \
  --conf spark.ui.enabled=false \
  --conf spark.eventLog.enabled=true \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --conf spark.eventLog.dir=/tmp \
  local:///shr/jars/stream_2_fat.jar

kafka
kafka-console-consumer.sh \
  --bootstrap-server my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092 \
  --topic iex-topic \
  --from-beginning

spark-kafka-script
spark-submit   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 --master k8s://$(minikube ip):8443   --deploy-mode client   --name pf-demo9   --conf spark.kubernetes.container.image=pyspark:v3.5.2.3  --conf spark.kubernetes.context=minikube   --conf spark.kubernetes.driver.pod.name=pf-demo9   --conf spark.kubernetes.namespace=spark   --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark   --conf spark.ui.enabled=false   --conf spark.eventLog.enabled=true   --conf spark.eventLog.dir=/tmp   spark_kafka_test.py


--helm output for spark
kumararpita@kumararpita-OMEN-Laptop-15-en0xxx:~/alpaca_stream_ingestion/minikube$ helm install my-spark oci://registry-1.docker.io/bitnamicharts/spark -f spark-values.yaml 
Pulled: registry-1.docker.io/bitnamicharts/spark:10.0.2
Digest: sha256:34c89a3c9a6efc9e3c112584c3dfab1cb5f6b096233a9db0348e90ef7105f816
NAME: my-spark
LAST DEPLOYED: Sun Jul 13 22:13:24 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: spark
CHART VERSION: 10.0.2
APP VERSION: 4.0.0

Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.

** Please be patient while the chart is being deployed **

1. Get the Spark master WebUI URL by running these commands:

  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[?(@.name=='http')].nodePort}" services my-spark-master-svc)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

2. Submit an application to the cluster:

  To submit an application to the cluster the spark-submit script must be used. That script can be
  obtained at https://github.com/apache/spark/tree/master/bin. Also you can use kubectl run.

  Run the commands below to obtain the master IP and submit your application.

  export EXAMPLE_JAR=$(kubectl exec -ti --namespace default my-spark-worker-0 -- find examples/jars/ -name 'spark-example*\.jar' | tr -d '\r')
  export SUBMIT_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[?(@.name=='cluster')].nodePort}" services my-spark-master-svc)
  export SUBMIT_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")

  kubectl run --namespace default my-spark-client --rm --tty -i --restart='Never' \
    --image docker.io/bitnami/spark:4.0.0-debian-12-r2 \
    -- spark-submit --master spark://$SUBMIT_IP:$SUBMIT_PORT \
    --class org.apache.spark.examples.SparkPi \
    --deploy-mode cluster \
    $EXAMPLE_JAR 1000

** IMPORTANT: When submit an application the --master parameter should be set to the service IP, if not, the application will not resolve the master. **




WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
  - master.resources
  - worker.resources
