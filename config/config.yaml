kafka:
  # bootstrap_servers: "instance-20250325-162745:9095"
  # bootstrap_servers: "localhost:9092"
  bootstrap_servers: "my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"
  bootstrap_servers_local: "192.168.49.2:32100"
  topics:
    raw_trade: "iex-topic-1"
    flattened_trade: "iex-topic-1-flattened"
  consumer:
    group_id: "offset-checker"
    enable_auto_commit: false

# Kafka Connect configuration
kafka_connect:
  image:
    repository: "localhost:5000/kafka-connect-s3"
    tag: "latest"
  replicas: 1
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  s3_sink:
    enabled: true
    bucket: "archive"
    endpoint: "http://minio-single-hl.minio-tenant.svc:9000"
    region: "us-east-1"
    format: "jsonl"
    compression: "gzip"
    max_records: 1000
    flush_interval_ms: 60000

alpaca:
  key: ""  # Set via environment variable ALPACA_KEY
  secret: ""  # Set via environment variable ALPACA_SECRET
  websocket_uri: "wss://stream.data.alpaca.markets/v2/iex"
  symbols: ["*"]
  timeout: 120
  max_retries: 5
  backoff_max: 120

spark:
  warehouse_path: "gs://alpaca-streamer/warehouse_poc"
  jars:
    - "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1"
    - "org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.8.1"
  processing_time: "60 seconds"
  iceberg:
    catalog: "spark_catalog"
    catalog_type: "hadoop"
  app_name: KafkaToIcebergAndBQStreamer
  spark_catalog: org.apache.iceberg.spark.SparkCatalog
  spark_catalog_type: hadoop

gcp:
  project_id: "coe-landing-zone-18-ats-2246"
  region: "us-east1"
  cluster_name: "alpaca-streamer"
  storage:
    bucket: "alpaca-streamer"
    checkpoints_prefix: "checkpoints/a"
    warehouse_prefix: "warehouse_poc"
  bigquery:
    table: "coe-landing-zone-18-ats-2246.alpaca_data.raw_alpaca_spark_stream"

logging:
  level: "INFO"
  name: "websocket-kafka-logger"
  mode: "stdout"

pinot:
  #controller_url is not needed if dynamically detected which is happeining now in the setup python script
  controller_url: "http://192.168.49.2:30900"  # NodePort for external access (no port-forward needed)
  schema_file: "load/schema.json"
  table_file: "load/table.json"

# MinIO configuration
minio:
  minikube_ip: "192.168.49.2"  # Update this for your Minikube instance
  rootUser: "minio"
  rootPassword: "minio123"
  storage:
    size: 100Gi
    path: /mnt/minio  # Local mount path on Minikube node
  browser: "on"

# System dependencies configuration
dependencies:
  docker_version: "5:28.5.1-1~ubuntu.25.10~questing"
  minikube_version: "v1.36.0"
  kubectl_version: "v1.34.0"
  helm_version: "v3.19.0"
  java_version: "openjdk-17-jdk"
  uv_version: "0.9.2"

# Installation directory configuration
directories:
  state_dir: "$HOME/.alpaca_infra_state"
  minikube_mount_dir: "/nvmewd/minikube_mount"
  minikube_mount_minio: "/nvmewd/minikube_mount/minio"
  minikube_mount_shr: "/nvmewd/minikube_mount/shr"

# Minikube resource configuration
minikube:
  cpu: 8
  memory: 14999  # in MB